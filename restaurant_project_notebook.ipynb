{"cells":[{"cell_type":"markdown","id":"db1ac0ac-0b8f-44de-ac8d-e963e4e23aa7","metadata":{"id":"db1ac0ac-0b8f-44de-ac8d-e963e4e23aa7"},"source":["# Use OpenAI CLiP, LangGraph, & RAG to Generate Competitive Restaurant Insights\n","\n","\n","## Summary\n","In this article, we are going to provide you with a set of tools, that will allow you to analyze restaurants in your neighbourhood and use that information to your advantage. Even though it may seem this is only for individuals who may be interested in seeking popular dishes, it also provides valuable information for businesses about their competition as well as insights into their own dining services.\n","\n","To achieve this, we are going to utilize [CLIP](https://www.activeloop.ai/resources/glossary/open-ai-cli-p/), which is a very capable model as it can project many different kinds of modalities into a common vector space. In case you would be interested in a introduction to this transformer-based model and the other technologies we utilize in our guide, you could also follow our official documentation for [image similarity search](https://docs.activeloop.ai/example-code/tutorials/vector-store/image-similarity-search) and [vector store in langchain](https://docs.activeloop.ai/example-code/tutorials/vector-store/deep-lake-vector-store-in-langchain). Since our data will consist of reviews from Google Maps, we will focus only on textual and image modalities. This is especially powerful in combination with [DeepLake](https://www.activeloop.ai/), which is a multimodal vector database capable of efficient storage of both of these. In particular, we will use it for vector search, in which we will extract image and text reviews that are the most relevant for a particular task.\n","\n","Additionally, to further enhance our capabilities, we employed LangGraph, a library for building stateful, multi-actor applications with LLMs, built on top of LangChain.\n","\n","Overall, this will allow you to extract information from publicly available reviews and utilize it for further decision-making. Be it finding an unexplored place or understanding what tastes people around share.\n","\n","## Steps\n","1. Selecting Location\n","2. Scraping the Restaurant Reviews\n","3. Ingesting the data into DeepLake Vector Store\n","4. Finding the Best Reviewed Restaurant with Your Favourite Food\n","5. Question Answering Based on Reviews\n","6. Categorizing Images to Restaurant Tags\n","7. Clustering All Images to Find the Most Popular Dishes\n","8. Summarizing the Findings"]},{"cell_type":"code","execution_count":null,"id":"JKUQQf2zyT-R","metadata":{"id":"JKUQQf2zyT-R"},"outputs":[],"source":["!pip install openai langchain deeplake apify-client torch open_clip_torch deeplake langchain_openai langgraph"]},{"cell_type":"code","execution_count":null,"id":"5dd9f752-81a3-434b-b4eb-b857a34425b9","metadata":{"id":"5dd9f752-81a3-434b-b4eb-b857a34425b9"},"outputs":[],"source":["# Import libraries\n","from apify_client import ApifyClient\n","import urllib.request\n","from langchain.vectorstores import DeepLake\n","from langchain.chains import RetrievalQA\n","from langchain.chat_models import ChatOpenAI\n","from langchain.llms import OpenAI\n","from langchain import PromptTemplate\n","from langchain.embeddings.base import Embeddings\n","import sys\n","\n","import torch\n","import os\n","import re\n","from tqdm import tqdm\n","from collections import defaultdict\n","from PIL import Image\n","import pandas as pd\n","import numpy as np\n","import base64\n","from io import BytesIO\n","from IPython.display import HTML\n","from sklearn.cluster import KMeans\n","\n","pd.set_option('display.max_colwidth', None)\n","\n","activeloop_token='YOUR_ACTIVELOOP_TOKEN'\n","os.environ['ACTIVELOOP_TOKEN'] = activeloop_token\n","\n","os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_TOKEN>'\n","os.environ['ACTIVELOOP_TOKEN'] = '<YOUR_ACTIVELOOP_TOKEN>'\n","os.environ['APIFY_API_TOKEN'] = '<YOUR_APIFY_API_TOKEN>'\n","\n","os.environ['TAVILY_API_KEY'] = \"<YOUR_TAVILY_API_TOKEN>\""]},{"cell_type":"markdown","id":"76f13973-a454-4f82-9e67-9ecf0defe808","metadata":{"id":"76f13973-a454-4f82-9e67-9ecf0defe808"},"source":["### Step 1: Selecting Location\n","First of all, you need to find the longitude and latitude of your location, from which the data will be scraped. This can be done in many ways, but the most straightforward is to open Google Maps, search for the place, right-click it and copy the coordinates. In our example, we will use the following location: `Crepevine, 300 Castro Street, Mountain View, CA 94041, United States of America`, which gives us `Latitude=37.3926` and `Longitude=-122.0800`. In case you would need to automate this, feel free to use geocoding via [Google Maps API](https://github.com/googlemaps/google-maps-services-python).\n"]},{"cell_type":"markdown","id":"5bbd7c78-591a-423e-bf11-764314dffccf","metadata":{"id":"5bbd7c78-591a-423e-bf11-764314dffccf"},"source":["### Step 2: Scraping the Restaurant Reviews\n","[Google Maps API](https://github.com/googlemaps/google-maps-services-python) offers many capabilities including information about places from a given location. They also have a generous free budget of `300$` every month, but it was a great disappointment that it is only able to extract 5 reviews for each restaurant, which is far from being enough for our task. Therefore, we recommend to utilize [Apify](https://apify.com/) actor. We experimented with other scrapers too, but unless you are willing to pay extra for faster scraping, it should be sufficient. You can see the `run_inputs` for more details, but this is the summary of our setup:\n","- all restaurants in 2 KM radius\n","- reviews only from 1.1.2022\n","- no limit for max images/texts/restaurants\n","\n","Apify provides a budget of `5$` per month and to give you an idea, here are the results of our run:\n","- total restaurants scraped: 130\n","- total scraping time: 75 minutes\n","- total costs: 2.3$\n","\n","This should be fast enough to scrape restaurants in your city, however, expanding to more locations might be problematic.\n"]},{"cell_type":"code","execution_count":null,"id":"7153cd80-dd8c-4ace-b6fc-ea4737eacc97","metadata":{"id":"7153cd80-dd8c-4ace-b6fc-ea4737eacc97"},"outputs":[],"source":["# Initialize the ApifyClient with your API token\n","client = ApifyClient(os.environ[\"APIFY_API_TOKEN\"])\n","\n","# Prepare the Actor input\n","run_input = {\n","  \"customGeolocation\": {\n","    \"type\": \"Point\",\n","    \"coordinates\": [\n","      \"-122.0800081\",\n","      \"37.39252210000001\"\n","    ],\n","    \"radiusKm\": 2\n","  },\n","  \"deeperCityScrape\": False,\n","  \"includeWebResults\": False,\n","  \"language\": \"en\",\n","  \"maxCrawledPlacesPerSearch\": 500,\n","  \"maxImages\": 200,\n","  \"maxReviews\": 200,\n","  \"oneReviewPerRow\": False,\n","  \"onlyDataFromSearchPage\": False,\n","  \"reviewsSort\": \"newest\",\n","  \"reviewsStartDate\": \"2022-01-01\",\n","  \"scrapeResponseFromOwnerText\": False,\n","  \"scrapeReviewId\": False,\n","  \"scrapeReviewUrl\": False,\n","  \"scrapeReviewerId\": False,\n","  \"scrapeReviewerName\": False,\n","  \"scrapeReviewerUrl\": False,\n","  \"searchStringsArray\": [\n","    \"restaurant\"\n","  ]\n","}\n","\n","# Run the Actor and wait for it to finish\n","run = client.actor(\"compass/crawler-google-places\").call(run_input=run_input)"]},{"cell_type":"markdown","id":"VtWkDCtJCBol","metadata":{"id":"VtWkDCtJCBol"},"source":["You can now proceed with the returned results or download them from your Apify account in csv format\n"]},{"cell_type":"code","execution_count":null,"id":"6i0V8brDCJGb","metadata":{"id":"6i0V8brDCJGb"},"outputs":[],"source":["choose = ''\n","if choose == 'csv':\n","  scraped_data = pd.read_csv(\"items.csv\")\n","  scraped_data = scraped_data.to_dict()\n","else:\n","  scraped_data = client.dataset(run['defaultDatasetId']).list_items().items"]},{"cell_type":"markdown","id":"52b3cafd-e08b-4846-8d36-099f03fd05a6","metadata":{"id":"52b3cafd-e08b-4846-8d36-099f03fd05a6"},"source":["After we scrape the data, it is necessary to define a function to extract reviews and other parameters. Since the scraper only extracts the URL link, it is useful to save the images during the first run and save them locally. This is useful because during our experiments we encountered problems when some of the images were no longer available and we needed to check each URL separately prior to ingesting the data into DeepLake. Their format was different from other images which caused issues during ingestion. Nevertheless, it is likely that there is a more efficient way to avoid it.\n","\n","\n","The image requesting is quite slow as well and in our experiments it took around 70 minutes to process the total of 7813 images."]},{"cell_type":"code","execution_count":null,"id":"3e6e8dcb-d53c-4399-bbd8-df96d9b511a7","metadata":{"id":"3e6e8dcb-d53c-4399-bbd8-df96d9b511a7"},"outputs":[],"source":["def review_mapping_function(item, save_images=False):\n","    title = item[\"title\"]\n","    text_dict = defaultdict(list)\n","    image_dict = defaultdict(list)\n","    tag_dict = defaultdict(list)\n","    image_folder = \"images/\" + title\n","    image_folder = image_folder.replace('|', '')\n","    if (not os.path.exists(image_folder)) and save_images:\n","        os.makedirs(image_folder)\n","\n","    for tag in item['reviewsTags']:\n","        tag_dict['metadata'] += [{'title': title}]\n","        tag_dict['tags'] += [tag['title']]\n","    for tag in ['interior', 'menu', 'drink']:\n","        tag_dict['metadata'] += [{'title': title}]\n","        tag_dict['tags'] += [tag]\n","\n","    for idx, r in enumerate(item[\"reviews\"]):\n","        text = r[\"textTranslated\"]\n","        # if text was originally in english, textTranslated is None\n","        if text is None:\n","            text = r[\"text\"]\n","        image = r[\"reviewImageUrls\"]\n","\n","        if text:\n","            metadata = {\n","                \"title\": title,\n","                \"review_id\": idx,\n","                \"likes\": r['likesCount'],\n","                \"stars\": r['stars'],\n","            }\n","            text_dict[\"text\"].append(text)\n","            text_dict[\"metadata\"].append(metadata)\n","\n","        if image:\n","            j=0\n","            for image_url in image:\n","                image_path = f\"{image_folder}/{j}.jpg\"\n","                metadata = {\n","                    \"title\": title,\n","                    \"review_id\": idx,\n","                    \"likes\": r['likesCount'],\n","                    \"stars\": r['stars'],\n","                }\n","                image_dict[\"metadata\"].append(metadata)\n","                # change image size\n","                image_url = re.sub('=w[0-9]+-h[0-9]+-', '=w512-h512-', image_url)\n","                if save_images:\n","                    urllib.request.urlretrieve(image_url, image_path)\n","\n","                if os.path.isfile(image_path):\n","                    # image was saved, we can just load it with path\n","                    image_dict[\"image\"].append(image_path)\n","                    j+=1\n","                else:\n","                    # image not saved, need to put url and request it later\n","                    image_dict[\"image\"].append(image_url)\n","\n","    return {'text_dict': text_dict, 'image_dict': image_dict, 'tag_dict': tag_dict}"]},{"cell_type":"code","execution_count":null,"id":"accfb771-f09c-4a63-add8-c48b2fdcf232","metadata":{"id":"accfb771-f09c-4a63-add8-c48b2fdcf232"},"outputs":[],"source":["reviews = [review_mapping_function(item) for item in scraped_data]"]},{"cell_type":"code","execution_count":null,"id":"2c5ed8b1-c9ca-4651-932c-98a33b2423a0","metadata":{"id":"2c5ed8b1-c9ca-4651-932c-98a33b2423a0"},"outputs":[],"source":["# extract reviews\n","reviews = [review_mapping_function(item) for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items()]\n","\n","# aggregate them into a single dictionary\n","text_dict_concat = defaultdict(list)\n","image_dict_concat = defaultdict(list)\n","tag_dict_concat = defaultdict(list)\n","\n","for r in reviews:\n","    for key in r['text_dict'].keys():\n","        text_dict_concat[key] += r['text_dict'][key]\n","    for key in r['image_dict'].keys():\n","        image_dict_concat[key] += r['image_dict'][key]\n","    for key in r['tag_dict'].keys():\n","        tag_dict_concat[key] += r['tag_dict'][key]"]},{"cell_type":"markdown","id":"7d49a4fa-7838-4a75-b59f-80bab57fb06e","metadata":{"id":"7d49a4fa-7838-4a75-b59f-80bab57fb06e"},"source":["### 3) Ingesting the data into DeepLake Vector Store\n","Initially, we experimented with various setups of the vector database. Even though the DeepLake is capable of saving text and image tensors in the same database, it gets quite complex as one review can have none or a single textual message, while images can range from zero up to as many as the reviewers add. If we decide to put all of them in the same database, it might look convenient but we end up duplicating the textual messages. As we did not find a particular use case that would benefit from this and it only resulted in a more complicated similarity search, we created two separate databases. One storing images from reviews and the other textual reviews, each with a different call to our custom embedding function. Additionally, we also introduced a third Deep Lake Vector Store with tags from each restaurant, which will be particularly useful during categorization in step 6.\n","\n","Also note that before ingesting the data, it is a common practice to split the text into documents. However, as Google map reviews are limited to 4096 characters (around 700 words), it is not necessary."]},{"cell_type":"code","execution_count":null,"id":"bbltZi3uEBjI","metadata":{"id":"bbltZi3uEBjI"},"outputs":[],"source":["activeloop_ord_id = 'YOUR_ACTIVELOOP_ORG'"]},{"cell_type":"code","execution_count":null,"id":"1c0a0ca1-5ea2-42f4-8814-fa9f8e12e053","metadata":{"id":"1c0a0ca1-5ea2-42f4-8814-fa9f8e12e053"},"outputs":[],"source":["from deeplake import VectorStore\n","\n","overwrite = False\n","\n","# Create empty database for texts\n","reviews_path_texts = f'hub://{activeloop_ord_id}/reviews-texts'\n","reviews_texts = VectorStore(\n","    path = reviews_path_texts,\n","    tensor_params = [\n","        {'name': 'text', 'htype': 'text'},\n","        {'name': 'embedding', 'htype': 'embedding'},\n","        {'name': 'metadata', 'htype': 'json'}\n","    ],\n","    overwrite = overwrite\n",")\n","# Create empty database for images\n","reviews_path_images = f'hub://{activeloop_ord_id}/reviews-images'\n","reviews_images = VectorStore(\n","    path = reviews_path_images,\n","    tensor_params = [\n","        {'name': 'image', 'htype': 'image', 'sample_compression': 'png'},\n","        {'name': 'embedding', 'htype': 'embedding'},\n","        {'name': 'metadata', 'htype': 'json'}\n","    ],\n","    overwrite = overwrite\n",")\n","# Create empty database for tags\n","reviews_path_tags = f'hub://{activeloop_ord_id}/restaurants-tags'\n","restaurants_tags = VectorStore(\n","    path = reviews_path_tags,\n","    tensor_params = [\n","        {'name': 'tag', 'htype': 'text'},\n","        {'name': 'embedding', 'htype': 'embedding'},\n","        {'name': 'metadata', 'htype': 'json'}\n","    ],\n","    overwrite = overwrite\n",")\n"]},{"cell_type":"markdown","id":"36529d2e-7b11-481b-9fbf-e01631f46087","metadata":{"id":"36529d2e-7b11-481b-9fbf-e01631f46087"},"source":["Now, let's define the custom OpenCLIP embedding function, which is a wrapper around [CLIP](https://github.com/openai/CLIP). Since the model needs to explicitly set the modality of input, it has two call options: 1) for text embedding and 2) for image embedding. It is quite important to set up CUDA and run the predictions on GPU, as the performance on the CPU is quite slow."]},{"cell_type":"code","execution_count":null,"id":"2abc6b19-b920-4239-98f7-5585500b234f","metadata":{"id":"2abc6b19-b920-4239-98f7-5585500b234f"},"outputs":[],"source":["from typing import Any, Dict, List\n","\n","from langchain.pydantic_v1 import BaseModel, root_validator\n","from langchain.schema.embeddings import Embeddings\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","\n","\n","class OpenCLIPEmbeddings(BaseModel, Embeddings):\n","    model: Any\n","    preprocess: Any\n","    tokenizer: Any\n","    # Select model: https://github.com/mlfoundations/open_clip\n","    model_name: str = \"ViT-H-14\"\n","    checkpoint: str = \"laion2b_s32b_b79k\"\n","\n","    @root_validator()\n","    def validate_environment(cls, values: Dict) -> Dict:\n","        \"\"\"Validate that open_clip and torch libraries are installed.\"\"\"\n","        try:\n","            import open_clip\n","\n","            # Fall back to class defaults if not provided\n","            model_name = values.get(\"model_name\", cls.__fields__[\"model_name\"].default)\n","            checkpoint = values.get(\"checkpoint\", cls.__fields__[\"checkpoint\"].default)\n","\n","            # Load model\n","            model, _, preprocess = open_clip.create_model_and_transforms(\n","                model_name=model_name, pretrained=checkpoint\n","            )\n","            tokenizer = open_clip.get_tokenizer(model_name)\n","            values[\"model\"] = model\n","            values[\"preprocess\"] = preprocess\n","            values[\"tokenizer\"] = tokenizer\n","\n","        except ImportError:\n","            raise ImportError(\n","                \"Please ensure both open_clip and torch libraries are installed. \"\n","                \"pip install open_clip_torch torch\"\n","            )\n","        return values\n","\n","    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n","        text_features = []\n","\n","        self.model.to('cuda')\n","\n","        tokenized_text = self.tokenizer(texts)\n","        # for text in texts:\n","        #     # Tokenize the text\n","        #     tokenized_text = self.tokenizer(text)\n","\n","        with torch.no_grad(), torch.cuda.amp.autocast():\n","            # Encode the text to get the embeddings\n","            embeddings_tensor = self.model.encode_text(tokenized_text.to('cuda'))\n","\n","        # Normalize the embeddings\n","        norm = embeddings_tensor.norm(p=2, dim=1, keepdim=True)\n","        normalized_embeddings_tensor = embeddings_tensor.div(norm)\n","\n","        # Convert normalized tensor to list and add to the text_features list\n","        embeddings_list = normalized_embeddings_tensor.squeeze(0).tolist()\n","        text_features.append(embeddings_list)\n","\n","        return text_features\n","\n","    def embed_query(self, text: str) -> List[float]:\n","        return self.embed_documents([text])[0]\n","\n","    def embed_image(self, uri: str) -> List[float]:\n","        return self.embed_images([uri])[0]\n","\n","    def embed_images(self, uris: List[str]) -> List[List[float]]:\n","\n","        try:\n","            from PIL import Image as _PILImage\n","        except ImportError:\n","            raise ImportError(\"Please install the PIL library: pip install pillow\")\n","\n","        # Open images directly as PIL images\n","        pil_images = []\n","        for uri in uris:\n","          pil_images.append(Image.open(uri))\n","\n","        self.model.to('cuda')\n","\n","        # Preprocess the image for the model\n","        preprocessed_image = [self.preprocess(pil_image) for pil_image in pil_images]\n","\n","        with torch.no_grad(), torch.cuda.amp.autocast():\n","            # Encode the image to get the embeddings\n","            embeddings_tensor = self.model.encode_image(torch.stack(preprocessed_image).to('cuda'))\n","\n","        # Normalize the embeddings tensor\n","        norm = embeddings_tensor.norm(p=2, dim=1, keepdim=True)\n","        normalized_embeddings_tensor = embeddings_tensor.div(norm)\n","\n","        # Convert tensor to list and add to the image_features list\n","        embeddings_list = normalized_embeddings_tensor.squeeze(0).tolist()\n","\n","        return embeddings_list"]},{"cell_type":"markdown","id":"e442be76-b8f8-414f-bd88-dfda68aeda90","metadata":{"id":"e442be76-b8f8-414f-bd88-dfda68aeda90"},"source":["Here, we proceed to ingest the scraped data into DeepLake. Note that it is important to set `ingestion_batch_size` appropriately for your GPU capacity, to avoid running out of memory during embedding prediction."]},{"cell_type":"code","execution_count":null,"id":"625734de-96dc-4abc-ab62-f1fb87371ea6","metadata":{"id":"625734de-96dc-4abc-ab62-f1fb87371ea6"},"outputs":[],"source":["# loading the model\n","clip = OpenCLIPEmbeddings(model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\")"]},{"cell_type":"code","source":["import requests\n","from PIL import Image\n","from io import BytesIO\n","\n","def save_image_from_url(url, restaurant_idx):\n","    # Download the image\n","    if not os.path.exists(\"images\"):\n","        os.mkdir(\"images\")\n","\n","    response = requests.get(url)\n","    image = Image.open(BytesIO(response.content))\n","    path_image = f\"images/{restaurant_idx}.png\"\n","    image.save(path_image, 'PNG')\n","    return path_image"],"metadata":{"id":"BtBvhmzSguLY"},"id":"BtBvhmzSguLY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","all_image_urls=image_dict_concat['image']\n","path_images = []\n","for image_idx, image_url in tqdm(enumerate(all_image_urls), total=len(all_image_urls)):\n","    try:\n","        path_images.append(save_image_from_url(image_url, image_idx))\n","    except Exception:\n","        continue"],"metadata":{"id":"MqkaeMuNgzxE"},"id":"MqkaeMuNgzxE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# texts\n","reviews_texts.add(\n","    text = text_dict_concat['text'],\n","    metadata = text_dict_concat['metadata'],\n","    embedding_function = clip.embed_documents,\n","    embedding_data = text_dict_concat['text'],\n","    embedding_tensor=\"embedding\",\n",")\n","# images\n","reviews_images.add(\n","    image = path_images,\n","    metadata = image_dict_concat['metadata'],\n","    embedding_function = clip.embed_images,\n","    embedding_data = path_images,\n","    embedding_tensor=\"embedding\",\n",")\n","# tags\n","restaurants_tags.add(\n","    tag = tag_dict_concat['tags'],\n","    metadata = tag_dict_concat['metadata'],\n","    embedding_function = clip.embed_documents,\n","    embedding_data = tag_dict_concat['tags'],\n","    embedding_tensor=\"embedding\",\n",")"],"metadata":{"id":"nerMBv8LxCuZ"},"id":"nerMBv8LxCuZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"56650a56-90f8-4758-8bd3-419b72282378","metadata":{"id":"56650a56-90f8-4758-8bd3-419b72282378"},"source":["In the case of 9607 textual reviews this took around 30 minutes, in the case of 7813 images this was around 1.5 hours and for tags it was under 5 minutes. This Was given mostly by the long inference time of the OpenCLIP model."]},{"cell_type":"markdown","id":"8c86beec-af36-4d73-be27-5cf3d7a090bb","metadata":{"id":"8c86beec-af36-4d73-be27-5cf3d7a090bb"},"source":["### 4) Finding the Best Reviewed Restaurant with Your Favourite Food\n","Finally, it's time to get some useful insights into our embedded dataset! We start by finding the 200 most relevant texts and images for search input `burger`."]},{"cell_type":"code","execution_count":null,"id":"31c83984-dc55-48aa-9bf5-ca72c2675e48","metadata":{"id":"31c83984-dc55-48aa-9bf5-ca72c2675e48"},"outputs":[],"source":["search = 'burger'\n","\n","text_search_results = reviews_texts.search(\n","    embedding_data = [search],\n","    embedding_function = clip.embed_documents,\n","    k=200,\n",")\n","image_search_results = reviews_images.search(\n","    embedding_data = [search],\n","    embedding_function = clip.embed_documents,\n","    k=200,\n",")"]},{"cell_type":"markdown","id":"ENKXJcX8tJmD","metadata":{"id":"ENKXJcX8tJmD"},"source":["\n","Another way to get results from deep lake is via TQL. It stands for Total Quality Logistics, which is a third-party logistics provider that connects shippers with carriers to move freight. They offer services such as truckload, LTL (less than truckload), intermodal, and supply chain solutions. TQL is one of the largest freight brokerage firms in the United States."]},{"cell_type":"code","execution_count":null,"id":"VT9oRPF2tfDt","metadata":{"id":"VT9oRPF2tfDt"},"outputs":[],"source":["query_emb = clip.embed_documents([search])"]},{"cell_type":"code","execution_count":null,"id":"m66o77QQtlPD","metadata":{"id":"m66o77QQtlPD"},"outputs":[],"source":["query_emb_str = \"ARRAY[\"+\",\".join([f\"{emb}\" for emb in query_emb[0]])+\"]\""]},{"cell_type":"code","execution_count":null,"id":"3mo4RyeZtl1V","metadata":{"id":"3mo4RyeZtl1V"},"outputs":[],"source":["info_to_scrape = 10\n","tql_images = f\"select image, metadata, id, score from (select *, cosine_similarity(embedding, {query_emb_str}) as score where cosine_similarity(embedding, {query_emb_str}) >0.2 order by metadata['rating'] desc limit {info_to_scrape})\"\n","tql_reviews = f\"select metadata, id, score from (select *, cosine_similarity(embedding, {query_emb_str}) as score where cosine_similarity(embedding, {query_emb_str}) >0.2 order by metadata['rating'] desc limit {info_to_scrape})\""]},{"cell_type":"code","execution_count":null,"id":"7wx1codKtoku","metadata":{"id":"7wx1codKtoku"},"outputs":[],"source":["image_search_results_tql = reviews_images.search(query=tql_images)\n","text_search_results_tql = reviews_texts.search(query=tql_reviews)"]},{"cell_type":"markdown","id":"WbBoQ5Pwtxwo","metadata":{"id":"WbBoQ5Pwtxwo"},"source":["Now we can aggregate the results"]},{"cell_type":"code","execution_count":null,"id":"a1496149-577f-4fba-9102-d53cdacef85a","metadata":{"id":"a1496149-577f-4fba-9102-d53cdacef85a"},"outputs":[],"source":["# aggregating the results\n","results = defaultdict(lambda: defaultdict(list))\n","\n","for md, img, id in zip(image_search_results['metadata'], image_search_results['image'],image_search_results['id']):\n","    results[md['title']]['images'].append(img)\n","    results[md['title']]['image_likes'].append(md['likes'])\n","    results[md['title']]['image_stars'].append(md['stars'])\n","    results[md['title']]['image_review_ids'].append(md['review_id'])\n","    results[md['title']]['image_ids'].append(id)\n","\n","\n","for md, txt in zip(text_search_results['metadata'], text_search_results['text']):\n","    results[md['title']]['texts'].append(txt)\n","    results[md['title']]['text_likes'].append(md['likes'])\n","    results[md['title']]['text_stars'].append(md['stars'])\n","    results[md['title']]['text_review_ids'].append(md['review_id'])"]},{"cell_type":"markdown","id":"3198cff4-acf0-4893-8a3b-cee4a8345e55","metadata":{"id":"3198cff4-acf0-4893-8a3b-cee4a8345e55"},"source":["Now let's summarize the text reviews. For this, we will use a simple prompt template that extracts a summary of keywords from a list of reviews and also includes an example. Since the reviews are typically short messages, we can only concatenate each set together and do not need to chain the calls with tools that are offered by [LangChain](https://python.langchain.com/docs/get_started/introduction.html). If you're interested in delving deeper into LangChain and exploring its capabilities further, we invite you to explore our comprehensive guide available at this [link](https://www.activeloop.ai/resources/langchain/)."]},{"cell_type":"code","execution_count":null,"id":"5b0e5b11-0e5e-432b-bea9-428f3150937a","metadata":{"id":"5b0e5b11-0e5e-432b-bea9-428f3150937a"},"outputs":[],"source":["llm = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.5)"]},{"cell_type":"code","execution_count":null,"id":"a20f77b1-c06c-4623-a213-8b49a0c92172","metadata":{"id":"a20f77b1-c06c-4623-a213-8b49a0c92172"},"outputs":[],"source":["prompt_template = \"\"\"You are provided with a list of {search} reviews. Summarize what customers write about it:\n","\n","Example:\n","List of {search} reviews:\n","Great spicy Burger !\\nThe burger is solid and delicious. Just be aware that it's high in calories (1100 calories!).\\nVery good food, I would recommend to the burger lovers out there.\\nThe burgers here are pretty solid\\nthey also have a rotating beer top which has some good variety\\nFantastic food\\nBest Burgers In Town!!!\\nGreat food\\nDELICIOUS! BISON BURGER IS THE BEST\"\n","delicious, cheap, good atmosphere, quick service, many options in menu.\n","\n","Keyword summary of the {search} reviews:\n","Spicy burger, Solid and delicious, Recommended for burger lovers, Good variety of beers, Fantastic food, Best burgers in town, Bison burger is delicious\n","\n","\n","List of {search} reviews:\n","{reviews}\n","\n","Keyword summary of the {search} reviews:\n","\"\"\""]},{"cell_type":"markdown","id":"bc392dfd-f909-4fbc-aeb0-74a8ecbfa6a2","metadata":{"id":"bc392dfd-f909-4fbc-aeb0-74a8ecbfa6a2"},"source":["To put it all together, we are going to loop through all of the 200 texts and 200 images that have the most similar embedding to `burger`, group them by restaurant `title` and define the following:\n","- `avg_txt_stars` - average stars on text messages for a given restaurant\n","- `n_texts` - number of text messages for a given restaurant\n","- `text_summary` - keyword summary based on all of the text messages for a given restaurant\n","- `avg_img_stars` - average stars on images for a given restaurant\n","- `n_images` - number of images for a given restaurant\n","- `img_in_text_perc` - percentage number of images selected along with their original text message (% of images connected with text by the `review_id`)\n","- `image_{i}` - image in top n most similar images\n","\n","To make the table more clear, we subset only the top 5 images (sorted by the similarity score) and the top 5 restaurants (sorted by the number of images)"]},{"cell_type":"markdown","source":["Since we will now use a langchain service we will use the Activeloop Vector Store in the following way"],"metadata":{"id":"QMTjcZ5Luo_y"},"id":"QMTjcZ5Luo_y"},{"cell_type":"code","source":["from langchain.vectorstores import DeepLake\n","\n","reviews_images_lc = DeepLake(\n","    dataset_path = reviews_path_images,\n",")\n","\n","reviews_texts_lc = DeepLake(\n","    dataset_path = reviews_path_texts,\n",")\n","\n","restaurants_tags_lc = DeepLake(\n","    dataset_path = reviews_path_tags,\n",")\n"],"metadata":{"id":"si-cxvCEvstf"},"id":"si-cxvCEvstf","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0176fadb-1cb6-43e7-b339-9584b50323df","metadata":{"id":"0176fadb-1cb6-43e7-b339-9584b50323df"},"outputs":[],"source":["df_1 = pd.DataFrame(columns=['title','info', 'text_summary'])\n","top_n = 5 # maximum number of images for each restaurant\n","n_restaurants = 5\n","const = 1\n","\n","i = 0\n","visualizer_images = []\n","for title, values in results.items():\n","    df_1.loc[i, 'title'] = title\n","\n","    info = {}\n","\n","    if len(values['texts']) > 0:\n","        weights = np.add(values['text_likes'], const)\n","        avg_txt_stars = round(np.average(values['text_stars'], weights=weights), 2)\n","        info['avg_txt_stars'] = avg_txt_stars\n","        n_texts = len(values['text_stars'])\n","        info['n_texts'] = n_texts\n","\n","        # set the prompt template\n","        PROMPT = PromptTemplate(\n","            template=prompt_template,\n","            input_variables=[\"reviews\"],\n","            partial_variables={\"search\": search},\n","        )\n","        reviews = \"\\n\".join(values['texts'])\n","        review_summary = llm(PROMPT.format(reviews=reviews, search=search))\n","        df_1.loc[i, 'text_summary'] = review_summary\n","\n","    if len(values['images']) > 0:\n","        weights = np.add(values['image_likes'], const)\n","        avg_img_stars = round(np.average(values['image_stars'], weights=weights), 2)\n","        info['avg_img_stars'] = avg_img_stars\n","        n_images = len(values['image_stars'])\n","        info['n_images'] = n_images\n","        df_1.loc[i, 'n_images'] = n_images\n","        visualizer_images += values['image_ids']\n","\n","        images_in_text = sum([i in values['text_review_ids'] for i in values['image_review_ids']])\n","        img_in_text_perc = round(images_in_text / len(values['image_review_ids']) * 100, 2)\n","        info['img_in_text_perc'] = img_in_text_perc\n","        sorted_images = [x for _, x in sorted(zip(values['image_likes'], values['images']), reverse=True, key=lambda x: x[0])]\n","        for j, img in enumerate(sorted_images):\n","            if j < top_n:\n","                df_1.loc[i, f'image_{j+1}'] = Image.fromarray(img).convert('RGB')\n","\n","    df_1.loc[i, 'info'] = str(info)\n","\n","    i+=1\n"]},{"cell_type":"markdown","id":"e44145d3-8c35-48f6-8742-87bd483f651a","metadata":{"id":"e44145d3-8c35-48f6-8742-87bd483f651a"},"source":["For better visualisation, we also define HTML formatters as inspired by [this notebook](https://www.kaggle.com/code/stassl/displaying-inline-images-in-pandas-dataframe) and render the HTML as generated by pandas."]},{"cell_type":"code","execution_count":null,"id":"fa0bdb3d-a286-46d0-9aac-19d29f03caf8","metadata":{"id":"fa0bdb3d-a286-46d0-9aac-19d29f03caf8"},"outputs":[],"source":["def get_thumbnail(path):\n","    i = Image.open(path)\n","    i.thumbnail((150, 150), Image.LANCZOS)\n","    return i\n","\n","def image_base64(im):\n","    if isinstance(im, str):\n","        im = get_thumbnail(im)\n","    with BytesIO() as buffer:\n","        im.save(buffer, 'jpeg')\n","        return base64.b64encode(buffer.getvalue()).decode()\n","\n","def image_formatter(im):\n","    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'\n","\n","def bullet_formatter(text):\n","    text = eval(text)\n","    l = '<div> <ul style=\"text-align: left;\">'\n","    for key, value in text.items():\n","        l += f\"\\n <li>{key}: {value}</li>\"\n","    l += \"\\n</ul></div>\"\n","    return l"]},{"cell_type":"code","execution_count":null,"id":"a9aac316-31fa-48f7-a29e-04efa67e018a","metadata":{"id":"a9aac316-31fa-48f7-a29e-04efa67e018a"},"outputs":[],"source":["# sort by n_images\n","df_1 = df_1.sort_values(by=['n_images'], ascending=False).drop(['n_images'],axis=1)\n","# render HTML\n","formatters = [None, bullet_formatter, None] + [image_formatter] * top_n\n","HTML(df_1[:n_restaurants].to_html(formatters=formatters, escape=False, col_space=[150]*df_1.shape[1]))"]},{"cell_type":"markdown","id":"362f18e3-94a0-4173-9d08-4528c0bb79d3","metadata":{"id":"362f18e3-94a0-4173-9d08-4528c0bb79d3"},"source":["The image results as you can see are very accurate, especially when you sort by vector similarity score. Summarizing textual reviews may seem to be sufficient too, but there's plenty of room for prompt engineering. Also, note that `img_in_text_perc` is very low and it was more of an experiment rather than proof that the results make sense. In other words, it is difficult to filter the images and text from the same review based on one embedding (in our example from `burger`)."]},{"cell_type":"markdown","id":"XsRfW-CYt5ee","metadata":{"id":"XsRfW-CYt5ee"},"source":["## Activeloop Visualizer\n","ActiveLoop Visualizer is a tool provided by [Activeloop](https://www.activeloop.ai/), a company specializing in creating databases for artificial intelligence applications. The Visualizer allows users to interact with and visualize data stored in their databases. It provides a user-friendly interface for exploring and understanding the data, making it easier for researchers and developers to work with large datasets effectively.\n","\n","For a detailed understanding of visualizer integration and how it enhances your ability to comprehend the relationships between tensors within a dataset, we encourage you to visit our technical documentation at this [link](https://docs.activeloop.ai/technical-details/visualizer-integration).\n","\n","If your dataset is not public you must also pass the private token in the request as follow:\n","\n","`iframe_url = f\"https://app.activeloop.ai/visualizer/iframe?url={reviews_path_images}&token=<YOUR_ACTIVELOOP_TOKEN>&query=`\n"]},{"cell_type":"code","execution_count":null,"id":"_vq317ePt6iO","metadata":{"id":"_vq317ePt6iO"},"outputs":[],"source":["def activeloop_visualizer(result:dict=None, list_images_id:list[str] = None):\n","  iframe_html = '<iframe src={url} width=\"570px\" height=\"400px\"/iframe>'\n","\n","  if result is not None:\n","    images_id = [el for el in result['id']]\n","  elif list_images_id is not None:\n","    images_id = list_images_id\n","  else:\n","    raise Exception(\"specify the images ids\")\n","\n","  images_id = str(images_id).strip(\"[]\")\n","  query = f\"select image where id in ({images_id})\"\n","  if activeloop_token is not None:\n","    iframe_url = f\"https://app.activeloop.ai/visualizer/iframe?url={reviews_path_images}&token={activeloop_token}&query=\"\n","  else:\n","    iframe_url = f\"https://app.activeloop.ai/visualizer/iframe?url={reviews_path_images}&query=\"\n","\n","  urls = [iframe_url + urllib.parse.quote(query)]\n","  html = iframe_html.format(url=urls[0])\n","  return HTML(html)"]},{"cell_type":"code","execution_count":null,"id":"4yTGRzAquiWd","metadata":{"id":"4yTGRzAquiWd"},"outputs":[],"source":["activeloop_visualizer(list_images_id=visualizer_images)"]},{"cell_type":"markdown","id":"13ea3185-5626-43a0-b933-91c681e36587","metadata":{"id":"13ea3185-5626-43a0-b933-91c681e36587"},"source":["### 5) Question Answering Based on Reviews\n","Of course, there are many use cases for [LangChain](https://python.langchain.com/docs/get_started/introduction.html) as well. In particular, you could utilize the text reviews as a custom document to answer any question. Note that as we embedded the tensors by OpenCLIP, we also need to set this function in the `retriever`. Also, as answering questions from the whole data does not make much sense in this case, we selected a single restaurant via the `filter` option in `search_kwargs`."]},{"cell_type":"code","execution_count":null,"id":"0d61300d-9c04-41d5-b414-64544ce9e1d8","metadata":{"id":"0d61300d-9c04-41d5-b414-64544ce9e1d8"},"outputs":[],"source":["retriever = reviews_texts_lc.as_retriever(\n","    search_type = \"similarity\",\n","    search_kwargs = {\n","        \"k\": 20,\n","        \"embedding_function\": clip.embed_documents,\n","        \"filter\": {'metadata': {'title': 'Taqueria La Espuela'}}\n","    }\n",")"]},{"cell_type":"markdown","id":"06e0d113-353c-413e-a2e9-7598fd6a4776","metadata":{"id":"06e0d113-353c-413e-a2e9-7598fd6a4776"},"source":["To see what reviews based on similarity search are provided to the LLM model to answer your question, we can inspect the `relevant_documents` as seen in the 5 examples below."]},{"cell_type":"code","execution_count":null,"id":"44d051ac-59f2-4bd7-a2e0-e910e315372c","metadata":{"id":"44d051ac-59f2-4bd7-a2e0-e910e315372c"},"outputs":[],"source":["query = 'What customers like about the restaurant?'\n","relevant_docs = retriever.get_relevant_documents([query])\n","relevant_docs[0:5]"]},{"cell_type":"markdown","id":"4701466b-fe54-438a-b64c-9f7820d3ea61","metadata":{"id":"4701466b-fe54-438a-b64c-9f7820d3ea61"},"source":["Now, let's define the QA retrieval and run your questions. But again, we stress the importance of further improvements of the prompt templates as they have significant effect on the results."]},{"cell_type":"code","execution_count":null,"id":"1464d0b8-d748-43a5-8581-21476775c8df","metadata":{"id":"1464d0b8-d748-43a5-8581-21476775c8df"},"outputs":[],"source":["qa = RetrievalQA.from_llm(llm, retriever=retriever)\n","qa.run([query])"]},{"cell_type":"code","execution_count":null,"id":"97bbe1f1-38b4-4a0a-a542-bfb2e0227f5e","metadata":{"id":"97bbe1f1-38b4-4a0a-a542-bfb2e0227f5e"},"outputs":[],"source":["query = 'What would customers improve about this restaurant?'\n","qa.run([query])"]},{"cell_type":"markdown","id":"b703f928-4e8a-482f-8187-1545c7b8b2d3","metadata":{"id":"b703f928-4e8a-482f-8187-1545c7b8b2d3"},"source":["### 6) Categorizing Images to Restaurant Tags\n","Typically, if your task is to categorize images, you need to train a model on a labelled set, which then limits the capabilities as it can predict only classes included in the training data. Here, however, we try to achieve similar results without training or finetuning any categorization model at all. Of course, the model isn't perfect, but the results seem to be pretty cool considering that we did not perform any finetuning on restaurant data and it is just the original OpenCLIP.\n","\n","Again, we filter only a single restaurant to make the predictions more clear."]},{"cell_type":"code","execution_count":null,"id":"8d60bc04-a661-4294-9f28-89ac28c92d75","metadata":{"id":"8d60bc04-a661-4294-9f28-89ac28c92d75"},"outputs":[],"source":["tensors = reviews_images_lc.vectorstore.search(\n","    return_tensors = ['image','embedding','id'],\n","    filter = {'metadata': {'title':'Taqueria La Espuela'}},\n",")"]},{"cell_type":"markdown","id":"ac84eb5a-f05e-4132-84dc-4eb970b30134","metadata":{"id":"ac84eb5a-f05e-4132-84dc-4eb970b30134"},"source":["Finally, we are going to utilize the third Deep Lake Vector Store which stores the restaurant tags along with their embeddings. The categorization is pretty straightforward as we are searching for the closest `tag` embedding for each of our images from the selected restaurant. After that, we sort them by similarity scores and display the top 10 images for each category.\n","\n","Notice that for practical reasons we also included additional tags `interior`, `menu` and `drink` for each restaurant as these were quite frequent images not included in the tags."]},{"cell_type":"code","execution_count":null,"id":"a5462f19-24ab-4c35-b84a-c3eca78bc5e8","metadata":{"id":"a5462f19-24ab-4c35-b84a-c3eca78bc5e8"},"outputs":[],"source":["df_2 = pd.DataFrame()\n","i_dict = defaultdict(lambda: 1)\n","n_images = 200\n","max_cols = 10\n","categories = []\n","scores = []\n","\n","for embedding in tensors['embedding']:\n","    closest = restaurants_tags_lc.vectorstore.search(\n","        embedding = embedding,\n","        k = 1,\n","        filter = {'metadata': {'title':'Taqueria La Espuela'}},\n","    )\n","    categories += [closest['tag'][0]]\n","    scores += [closest['score']]\n","\n","sorted_images = [x for _, x in sorted(zip(scores, tensors['image']), reverse=True)]\n","sorted_categories = [x for _, x in sorted(zip(scores, categories), reverse=True)]\n","\n","n = 0\n","for category, img in zip(sorted_categories, sorted_images):\n","    if n < n_images:\n","        i = i_dict[category]\n","        df_2.loc[category, f'image_{i}'] = Image.fromarray(img).convert('RGB')\n","        i_dict[category]+=1\n","        n+=1"]},{"cell_type":"markdown","id":"0600d1b9-fd64-43ea-ac96-2d1dd954d906","metadata":{"id":"0600d1b9-fd64-43ea-ac96-2d1dd954d906"},"source":["Again, rendering the formatted HTML."]},{"cell_type":"code","execution_count":null,"id":"9a18744c-8237-4dbf-b92a-5b8f2f2933ad","metadata":{"id":"9a18744c-8237-4dbf-b92a-5b8f2f2933ad"},"outputs":[],"source":["formatters = [image_formatter] * min(max_cols, df_2.shape[1])\n","HTML(df_2.iloc[:,:max_cols].to_html(formatters=formatters, escape=False))"]},{"cell_type":"markdown","id":"hrCYOyLTvwGS","metadata":{"id":"hrCYOyLTvwGS"},"source":["We can visualize the images in our Activeloop dataset"]},{"cell_type":"code","execution_count":null,"id":"jlUbqWeUvHXz","metadata":{"id":"jlUbqWeUvHXz"},"outputs":[],"source":["activeloop_visualizer(result=tensors)"]},{"cell_type":"markdown","id":"266698cc-e441-4cc9-a10f-303901381fcb","metadata":{"id":"266698cc-e441-4cc9-a10f-303901381fcb"},"source":["### 7) Clustering All Images to Find the Most Popular Dishes\n","\n","What if we want to group all of the images based on their similarity without any particular label to find the most popular meals? Of course, we can do that too! At the time of writing this article, DeepLake, unfortunately, does not support computing the cluster groups and extracting them. Anyways, it is currently on a road map and meanwhile, you can visualise them in the DeepLake UI that computes them on the fly or follow this guide that extracts the embeddings from Deep Lake Vector Store and calculates the clusters locally.\n","\n","We start by taking out 5000 images whose embedding is similar to `food`. This process is quite time-consuming since we are also extracting the images with metadata information."]},{"cell_type":"code","execution_count":null,"id":"2f1dd36f-7bf0-4ea5-9fc4-9ebb03935078","metadata":{"id":"2f1dd36f-7bf0-4ea5-9fc4-9ebb03935078"},"outputs":[],"source":["tensors = reviews_images_lc.vectorstore.search(\n","    return_tensors = ['metadata','image','embedding', 'id'],\n","    embedding_data = ['food'],\n","    embedding_function = clip.embed_documents,\n","    k = 5000,\n",")"]},{"cell_type":"markdown","id":"2919cac3-d411-437b-8403-1b7738959852","metadata":{"id":"2919cac3-d411-437b-8403-1b7738959852"},"source":["Then simply run the K-means clustering algorithm from `sklearn`. The number of clusters here is arbitrary as clustering is an unsupervised algorithm so you can play with other parameters too."]},{"cell_type":"code","execution_count":null,"id":"b3b32443-d40a-4d65-ab49-3bf1252916a8","metadata":{"id":"b3b32443-d40a-4d65-ab49-3bf1252916a8"},"outputs":[],"source":["n_clusters = 5\n","kmeans_model = KMeans(n_clusters = n_clusters)\n","clusters = kmeans_model.fit_predict(tensors['embedding']).tolist()"]},{"cell_type":"markdown","id":"ac1df9e4-1e78-4700-8cf0-b94402178998","metadata":{"id":"ac1df9e4-1e78-4700-8cf0-b94402178998"},"source":["We then create a simple data frame that aggregates information about clusters, stars and likes for each image and then select the top 10 images (sorted by similarity) from the top 5 clusters (sorted by average number of likes)."]},{"cell_type":"code","execution_count":null,"id":"ba14ab37-f8d0-48f6-b592-302a49641eae","metadata":{"id":"ba14ab37-f8d0-48f6-b592-302a49641eae"},"outputs":[],"source":["agg = pd.DataFrame()\n","df_3 = pd.DataFrame(columns=['cluster', 'avg_likes', 'n_images'])\n","max_cols = 10\n","max_rows = 5\n","cluster_visualizer_images = []\n","n = 0\n","for cluster, img, md, image_ids in zip(clusters, tensors['image'], tensors['metadata'], tensors['id']):\n","    agg.loc[n, 'cluster'] = cluster\n","    agg.loc[n, 'stars'] = md['stars']\n","    agg.loc[n, 'likes'] = md['likes']\n","    agg.loc[n, 'image'] = Image.fromarray(img).convert('RGB')\n","    agg.loc[n, 'image_ids'] = image_ids\n","    n += 1\n","\n","\n","agg = agg.sort_values(by=['likes'], ascending=False)\n","agg = agg.groupby('cluster').agg({'likes':['mean','count'], 'image': list,'image_ids': list})\n","agg = agg.sort_values(by=[('likes', 'mean')], ascending=False)\n","\n","r = 1\n","for index, row in agg.iterrows():\n","    if r <= max_rows:\n","        df_3.loc[r, 'cluster'] = int(index)\n","        df_3.loc[r, 'avg_likes'] = round(row['likes']['mean'], 2)\n","        df_3.loc[r, 'n_images'] = row['likes']['count']\n","        cluster_visualizer_images.append(row['image_ids'])\n","\n","        c=1\n","        for img in row['image']['list']:\n","            if c <= max_cols:\n","                df_3.loc[r, f'image_{c}'] = img\n","                c+=1\n","        r+=1"]},{"cell_type":"markdown","id":"83114f61-3be9-4b9d-b2f6-2f8876da462e","metadata":{"id":"83114f61-3be9-4b9d-b2f6-2f8876da462e"},"source":["*And* rendering it."]},{"cell_type":"code","execution_count":null,"id":"da142ef5-dd77-46d0-a30c-0d507abf1aaf","metadata":{"id":"da142ef5-dd77-46d0-a30c-0d507abf1aaf"},"outputs":[],"source":["formatters = [None, None, None] + [image_formatter] * max_cols\n","HTML(df_3.to_html(formatters=formatters, escape=False))"]},{"cell_type":"markdown","id":"285fda39-1d77-46b5-b652-bcc3b4342ff3","metadata":{"id":"285fda39-1d77-46b5-b652-bcc3b4342ff3"},"source":["As you can see, the food in each cluster is indeed quite similar. However, average likes might not be the appropriate metric to sort the most popular food as the number of likes on each image is typically low. If the cluster consists of many images, it is then more likely that we will not see it on top of this list."]},{"cell_type":"markdown","source":["It is possible to view the images of the different clusters directly on Activeloop by specifying the number of the cluster that interests us\n"],"metadata":{"id":"ABdFS2rLm7cx"},"id":"ABdFS2rLm7cx"},{"cell_type":"code","source":["cluster_number = 0\n","activeloop_visualizer(list_images_id=cluster_visualizer_images[cluster_number][0])"],"metadata":{"id":"LoNk07AAm-xx"},"id":"LoNk07AAm-xx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_number = 1\n","activeloop_visualizer(list_images_id=cluster_visualizer_images[cluster_number][0])"],"metadata":{"id":"6XNDQ-0mnFx8"},"id":"6XNDQ-0mnFx8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2>Introduction to LangGraph</h2>\n","LangGraph extends the LangChain Expression Language with the ability to coordinate multiple chains, or actors, across multiple steps of computation in a cyclic manner, inspired by Pregel and Apache Beam. The main use is for adding cycles to your LLM application, crucial for agent-like behaviors, where you call an LLM in a loop, asking it what action to take next. Specifically, we utilized Agent Supervisor within LangGraph, acting as a coordinator to delegate tasks between independent agents, orchestrating interactions and workflows efficiently."],"metadata":{"id":"J_KEqo9qHegq"},"id":"J_KEqo9qHegq"},{"cell_type":"markdown","source":["![Agent Supervisor](https://raw.githubusercontent.com/langchain-ai/langgraph/35188d9ed51ebbb0e2527f16068f2df50b62bd34/examples/multi_agent/img/supervisor-diagram.png)"],"metadata":{"id":"LkRr5svVit75"},"id":"LkRr5svVit75"},{"cell_type":"markdown","source":["Below, we will create an agent group, with an agent supervisor to help delegate tasks and to simplify the code in each agent node, we will use the AgentExecutor class from LangChain."],"metadata":{"id":"OpzUyug7jIe6"},"id":"OpzUyug7jIe6"},{"cell_type":"markdown","source":["##Create tools\n","\n","For this example, you will create an agent to search the web with a search engine and an agent to retrieve the review from Activeloop datasets. Define the tools they'll use below:"],"metadata":{"id":"Ca7NeZ-jjMkF"},"id":"Ca7NeZ-jjMkF"},{"cell_type":"code","source":["from typing import Annotated, List, Tuple, Union\n","from langchain_core.tools import tool\n","from langchain.agents import AgentExecutor, create_openai_tools_agent\n","from langchain_core.messages import BaseMessage, HumanMessage\n","from langchain_openai import ChatOpenAI\n","\n","def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n","    # Each worker node will be given a name and some tools.\n","    prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\n","                \"system\",\n","                system_prompt,\n","            ),\n","            MessagesPlaceholder(variable_name=\"messages\"),\n","            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n","        ]\n","    )\n","    agent = create_openai_tools_agent(llm, tools, prompt)\n","    executor = AgentExecutor(agent=agent, tools=tools)\n","    return executor\n"],"metadata":{"id":"19NKT5xGJhlc"},"id":"19NKT5xGJhlc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def agent_node(state, agent, name):\n","    result = agent.invoke(state)\n","    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"],"metadata":{"id":"2YI_xpL7NEFd"},"id":"2YI_xpL7NEFd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Create Agent Supervisor\n","It will use function calling to choose the next worker node OR finish processing."],"metadata":{"id":"mVAruEvVNI1p"},"id":"mVAruEvVNI1p"},{"cell_type":"code","source":["from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","#members = [\"Researcher\", \"QAAgent\"]\n","members = [\"Researcher\", \"Reviewer\"]\n","system_prompt = (\n","    \"You are a supervisor tasked with managing a conversation between the\"\n","    \" following workers:  {members}. Given the following user request,\"\n","    \" respond with the worker to act next. Each worker will perform a\"\n","    \" task and respond with their results and status. When finished,\"\n","    \" respond with FINISH.\"\n",")\n","# Our team supervisor is an LLM node. It just picks the next agent to process\n","# and decides when the work is completed\n","options = [\"FINISH\"] + members\n","# Using openai function calling can make output parsing easier for us\n","function_def = {\n","    \"name\": \"route\",\n","    \"description\": \"Select the next role.\",\n","    \"parameters\": {\n","        \"title\": \"routeSchema\",\n","        \"type\": \"object\",\n","        \"properties\": {\n","            \"next\": {\n","                \"title\": \"Next\",\n","                \"anyOf\": [\n","                    {\"enum\": options},\n","                ],\n","            }\n","        },\n","        \"required\": [\"next\"],\n","    },\n","}\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system_prompt),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","        (\n","            \"system\",\n","            \"Given the conversation above, who should act next?\"\n","            \" Or should we FINISH? Select one of: {options}\",\n","        ),\n","    ]\n",").partial(options=str(options), members=\", \".join(members))\n","\n","llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n","\n","supervisor_chain = (\n","    prompt\n","    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n","    | JsonOutputFunctionsParser()\n",")"],"metadata":{"id":"Fm8ryfNFNGhl"},"id":"Fm8ryfNFNGhl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Define the tools functions"],"metadata":{"id":"zejzyiCdPLPN"},"id":"zejzyiCdPLPN"},{"cell_type":"code","source":["@tool\n","def FindBestReviewFromQuery(search:str):\n","  \"\"\"\n","  Find the best review and return them\n","  \"\"\"\n","  text_search_results = reviews_texts_lc.vectorstore.search(\n","      embedding_data = [search],\n","      embedding_function = clip.embed_documents,\n","      k=200,\n","  )\n","  return text_search_results\n","\n","@tool\n","def QAToolFunction(query):\n","  \"\"\"\n","  Define the question answering model\n","  \"\"\"\n","  retriever = reviews_texts_lc.as_retriever(\n","    search_type = \"similarity\",\n","    search_kwargs = {\n","        \"k\": 20,\n","        \"embedding_function\": clip.embed_documents,\n","        \"filter\": {'metadata': {'title': 'Taqueria La Espuela'}}\n","    }\n","  )\n","  qa = RetrievalQA.from_llm(llm, retriever=retriever)\n","\n","  return qa.run([query])"],"metadata":{"id":"D0kwjO3KPQCt"},"id":"D0kwjO3KPQCt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Construct Graph\n","We're ready to start building the graph. Below, define the state and worker nodes using the function we just defined."],"metadata":{"id":"gQvQ5DDgPAIV"},"id":"gQvQ5DDgPAIV"},{"cell_type":"code","source":["import operator\n","from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n","import functools\n","\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langgraph.graph import StateGraph, END\n","from langchain_community.tools.tavily_search import TavilySearchResults\n","\n","# The agent state is the input to each node in the graph\n","class AgentState(TypedDict):\n","    # The annotation tells the graph that new messages will always\n","    # be added to the current states\n","    messages: Annotated[Sequence[BaseMessage], operator.add]\n","    # The 'next' field indicates where to route to next\n","    next: str\n","\n","tavily_tool = TavilySearchResults(max_results=5)\n","research_agent = create_agent(llm, [tavily_tool], \"You are a web researcher.\")\n","research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n","\n","reviewer_agent = create_agent(llm, [FindBestReviewFromQuery], \"You are a Review Retriever and take these from the Activeloop dataset\")\n","reviewer_node = functools.partial(agent_node, agent=reviewer_agent, name=\"Reviewer\")\n","\n","#qa_agent = create_agent(llm,[QAToolFunction], \"You can answer questions the user asks you\")\n","#qa_node = functools.partial(agent_node, agent=qa_agent, name=\"QAAgent\")\n","\n","workflow = StateGraph(AgentState)\n","workflow.add_node(\"Researcher\", research_node)\n","workflow.add_node(\"Reviewer\", reviewer_node)\n","workflow.add_node(\"supervisor\", supervisor_chain)"],"metadata":{"id":"cuhNOqKkNfgH"},"id":"cuhNOqKkNfgH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now connect all the edges in the graph."],"metadata":{"id":"TW1WI694VTw-"},"id":"TW1WI694VTw-"},{"cell_type":"code","source":["for member in members:\n","    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n","    workflow.add_edge(member, \"supervisor\")\n","# The supervisor populates the \"next\" field in the graph state\n","# which routes to a node or finishes\n","conditional_map = {k: k for k in members}\n","conditional_map[\"FINISH\"] = END\n","workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n","# Finally, add entrypoint\n","workflow.set_entry_point(\"supervisor\")\n","\n","graph = workflow.compile()"],"metadata":{"id":"miT0IfvhVQlo"},"id":"miT0IfvhVQlo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Invoke the team\n","With the graph created, we can now invoke it and see how it performs!"],"metadata":{"id":"NlXHPR4pWB4s"},"id":"NlXHPR4pWB4s"},{"cell_type":"markdown","source":["Invoke the Reviewer Agent"],"metadata":{"id":"iY9gWXWbQxL-"},"id":"iY9gWXWbQxL-"},{"cell_type":"code","source":["for s in graph.stream(\n","    {\n","        \"messages\": [\n","            HumanMessage(content=\"Retrieve restaurant reviews by the word 'burger'\")\n","        ]\n","    }\n","):\n","    if \"__end__\" not in s:\n","        print(s)\n","        print(\"----\")"],"metadata":{"id":"2OpTN_8nVWCX"},"id":"2OpTN_8nVWCX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Invoke the Researcher Agent"],"metadata":{"id":"qnGA7nu2QvID"},"id":"qnGA7nu2QvID"},{"cell_type":"code","source":["for s in graph.stream(\n","    {\n","        \"messages\": [\n","            HumanMessage(content=\"Find the best restaurant in Rome.\")\n","        ]\n","    }\n","):\n","    if \"__end__\" not in s:\n","        print(s)\n","        print(\"----\")"],"metadata":{"id":"HY0877MwWGJW"},"id":"HY0877MwWGJW","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"18a71cbc-ab6c-4aa2-9cb7-c4f6c8a23bdc","metadata":{"id":"18a71cbc-ab6c-4aa2-9cb7-c4f6c8a23bdc"},"source":["# Summarizing the Findings\n","To conclude what is and is not possible in the context of restaurant insights, the OpenCLIP embeddings are surprisingly accurate in not just recognizing food in general, but also the particular dish. In combination with DeepLake, it then provides valuable insights into the restaurant reviews and can help you better imagine what people enjoy eating in your neighbourhood. This can be especially helpful if the dining options are rich and it's difficult to check restaurants one-by-one. The biggest weakness, however, is the data preparation process which is highly time-consuming. To scrape, process and ingest data from 130 restaurants, the total runtime was around 4.5 hours, nevertheless, there are still ways to make this faster and more efficient.\n","\n","Overall, we see that the OpenCLIP embeddings are very powerful and can be very useful for LangChain as well, even though there is currently no integration. The highest potential we see in the unsupervised categorization and image search by text without any context, which as you could see worked pretty well and is far from being limited only to restaurant data.\n","\n","Additionally, LangGraph played a significant role in enhancing our capabilities. It provided a framework for building stateful, multi-actor applications with LLMs, allowing us to coordinate multiple chains across multiple steps of computation in a cyclic manner. This facilitated efficient task distribution and coordination, particularly through the use of Agent Supervisor, which delegated tasks between independent agents within the system, orchestrating interactions and workflows effectively.\n","\n","We hope that you find this article interesting and useful for your future projects and hopefully see you next time.\n","Have a good day!"]},{"cell_type":"markdown","source":["##FAQs:\n","\n","<h2 id=\"faq\">What is CLIP Model in AI?</h2>\n","CLIP is a neural network developed by OpenAI that connects text and images efficiently by learning visual concepts from natural language supervision. It utilizes a simple pre-training task where the model predicts which text snippet is paired with an image from a set of 32,768 options. This approach allows CLIP to recognize a wide range of visual concepts in images, enabling it to be applied to various visual classification tasks without the need for extensive labeled datasets. Unlike traditional deep learning models that rely on costly manually labeled datasets, CLIP learns from text-image pairs available on the internet, reducing the dependency on expensive data collection processes.\n","\n","<h2 id=\"faq\">What is LangGraph in LangChain?</h2>\n","LangGraph in LangChain is a library designed for building stateful, multi-actor applications with LLMs (Large Language Models). It is intended to be used with LangChain and extends the LangChain Expression Language to coordinate multiple chains or actors across multiple steps. LangGraph allows for adding cycles to LLM applications, particularly useful for agent-like behaviors where an LLM is called in a loop to determine the next action. LangGraph's main purpose is to enhance LLM applications with cycles, unlike LangChain, which is optimized for Directed Acyclic Graph (DAG) workflows.\n","\n","<h2 id=\"faq\">What is the difference between LangGraph and LangChain?</h2>\n","LangGraph and LangChain are related components within the LangChain framework. LangGraph is a library designed for building stateful, multi-actor applications with LLMs, focusing on coordinating multiple chains or actors across various steps. It extends the LangChain Expression Language to enable cycles in LLM applications, particularly useful for agent-like behaviors where actions are determined through iterative interactions.\n","On the other hand, LangChain is a broader framework for developing applications powered by language models. It offers composable tools, off-the-shelf chains, and integrations for working with language models. LangChain emphasizes context-awareness and reasoning capabilities, connecting language models to contextual sources and enabling them to reason based on provided context. In essence, LangGraph is more specialized in facilitating the creation of complex application structures involving multiple actors, while LangChain provides a comprehensive framework for developing diverse applications powered by language models.\n","\n","<h2 id=\"faq\">Is LangGraph Free?</h2>\n","LangGraph is an open-source library, which means it is freely available for anyone to use. You can access and use LangGraph without any cost, subject to the terms of its open-source license. However, it's essential to review the specific licensing terms associated with LangGraph to ensure compliance with its usage requirements."],"metadata":{"id":"Nz1wsQzruz-1"},"id":"Nz1wsQzruz-1"}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}